{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 제 4-2 강: 퍼셉트론 알고리즘\n",
    "\n",
    "## 학습 목표\n",
    "    - 퍼셉트론 알고리즘을 이해한다.\n",
    "    \n",
    "## 학습 내용\n",
    "    - 퍼셉트론 알고리즘\n",
    "    - 퍼셉트론 가중치 계산 \n",
    "    - 퍼셉트론 학습 전체 과정\n",
    "    - 퍼셉트론 알고리즘의 한계\n",
    "    - 퍼셉트론 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 퍼셉트론 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 알고리즘\n",
    "\n",
    "우리가 구하고자 하는 것은 입력값 $x$들을 분류해 낼 수 있는 가중치 $w$인데, 로젠블라트가 처음 제안한 학습 알고리즘은 다음과 같이 요약할 수 있습니다.  \n",
    "1. 가중치를 0 혹은 무작위 작은 수로 초기화 한다. \n",
    "2. 각 학습 자료$^{Training \\ Sample}$  $x^{(i)}$에 대해 다음을 실행한다. \n",
    "   - 출력 $\\hat{y}$를 계산한다.    즉 $\\hat{y} = h(\\mathbf{w^Tx})$\n",
    "   - 가중치$w_j$를 조정한다. 즉 $w_j := w_j + \\Delta w_j$\n",
    "   \n",
    "$x^{(i)}$ 에서 윗첨자 $(i)$는 입력되는 여러 학습자료에 하나씩 번호를 붙인 것입니다. 따라서 학습자료는 각각 $x^{(1)}, x^{(2)}$, ...이 됩니다.  $\\hat{y}$ 는 y hat(햇)으로 읽으며, 퍼셉트론이 계산한, 즉 퍼셉트론이 예측한$^{predicted}$ 예측값입니다.  $y$는 실제값 혹은 클래스 레이블(True Class Label)이라고 하며, 명시적인 정답을 말합니다. 실제값 혹은 클래스 레이블은 입력된 해당 학습자료가 출력해야 할 이미 알려진 참 값을 말합니다. $\\Delta$는 delta$^{델타}$라고 읽으며, 수학에서 \"차이\" 혹은 \"작은 값\"을 표시할 때 흔히 사용합니다. 기호 $:=$는 등호(=)가 아니며, 오른쪽 항을 계산하여 왼쪽 항에 설정(Assignment)한다는 개념입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 가중치 계산\n",
    "\n",
    "각 학습자료에 따라 계산을 하면서, 가중치를 조정하는 값은 다음의 방법을 따릅니다.  \n",
    "\n",
    "\\begin{align}\n",
    "   \\Delta w_j= \\eta (y^{(i)} - \\hat{y}^{(i)})x_j^{(i)}   \\tag{1} \n",
    "\\end{align}\n",
    "   \n",
    "여기서 $\\eta^{에타}$는 학습률은 나타내며 대개 0부터 1.0 사이의 상수입니다. 식(1)을 2차원의 예를 들어 각각의 경우를 풀어서 표기하면 다음과 같습니다. 여기서 주의할 것은 가중치를 조정할 때, 모든 값($w_0$, $w_1$, ..., $w_n$)을 동시에 조정해야 한다는 것입니다. \n",
    "\n",
    "\\begin{align}\n",
    "   \\Delta w_0 &= \\eta (y^{(i)} - \\hat{y}^{(i)})   \\\\\n",
    "   \\Delta w_1 &= \\eta (y^{(i)} - \\hat{y}^{(i)})x_1^{(i)}  \\\\   \n",
    "   \\Delta w_2 &= \\eta (y^{(i)} - \\hat{y}^{(i)})x_2^{(i)}   \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 퍼셉트론 예제를 다루기 전에 식(1)의 의미를 살펴보면 좋겠습니다. 식(1)을 관찰해보면, 예측값과 실제값(클래스 레이블)의 차이에 입력과 학습률을 곱한 값만큼 가중치를 조정하는 것입니다. 이것이 퍼셉트론 학습원칙의 핵심입니다. \n",
    "\n",
    "이 원칙이 어떻게 지켜지는지 간단한 테스트를 해봅시다. 활성화 함수 $h$가 양극성 계단 함수로, 1또는 -1만 반환한다고 해봅시다.\n",
    "\n",
    "\n",
    "실제값 y와 예측값 $\\hat{y}$이 같은 경우(다 같이1 혹은 다 같이 -1)를 식(1)에 대입하면, $\\Delta{w_j}$는 0가 되므로, $w_j$는 조정되지 않습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   \\Delta w_j &= \\eta (1^{(i)} - 1^{(i)})x_j^{(i)} = 0  \\\\\n",
    "   \\Delta w_j &= \\eta (-1^{(i)} - (-1^{(i)}))x_j^{(i)} = 0 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에, 클래스 레이블(참값) y와  예측한 값 $\\hat{y}$이 서로 다를 경우(하나는 1, 다른 하나는 -1)를 식(1)에 대입하면, $\\Delta{w_j}$는 0이 아니므로, $w_j$는 다음과 같이 커지거나 작아지도록 조정이 될 것입니다.  \n",
    " \n",
    " \\begin{align}\n",
    "   \\Delta w_j &= \\eta (1^{(i)} - (-1^{(i)}))x_j^{(i)} = \\eta (2)x_j^{(i)} \\\\\n",
    "   \\Delta w_j &= \\eta (-1^{(i)} - 1^{(i)})x_j^{(i)} = \\eta (-2)x_j^{(i)} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__예제 1__: 학습을 통해 다음의 가중치 $w$ 가 학습되었고, 예측값 $\\hat{y}$와 클래스 레이블(참값) $y$ 가 주어졌다고 가정합시다. 몇 번째 샘플의 가중치가 조정될까요?\n",
    "\n",
    "- $\\mathbf{\\hat{y}} = (\\hat{y_1}, \\hat{y_2}, \\hat{y_3}, \\hat{y_4}, \\hat{y_5}) = (1, -1, 1, 1, -1)$ \n",
    "- $\\mathbf{y} = ({y_1}, {y_2}, {y_3}, {y_4}, {y_5}) = (1, -1, -1, 1, -1)$ \n",
    "- $(x_1, x_2) = (0, 1)$\n",
    "\n",
    "(1) 1\n",
    "(2) 2\n",
    "(3) 3\n",
    "(4) 4\n",
    "(5) 5\n",
    "\n",
    "__예제풀이__: (3)\n",
    "\\begin{align}   \n",
    "    {y_1} - \\hat{y_1} = 0\\\\\n",
    "    {y_2} - \\hat{y_2} = 0\\\\\n",
    "    {y_3} - \\hat{y_3} = 2\\\\\n",
    "    {y_4} - \\hat{y_4} = 0\\\\\n",
    "    {y_5} - \\hat{y_5} = 0\\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 설명한 퍼셉트론 알고리즘을 도식화하면 다음과 같이 요약할 수 있습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/PerceptronAlgorithm.png?raw=true\" width=\"600\">\n",
    "<center>그림 3: 퍼셉트론 알고리즘</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 퍼셉트론 전체 학습 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론이 만들어지기 까지의 전체 과정을 요약하면 다음과 같습니다.\n",
    "1. 학습 자료들을 입력받는다.\n",
    "2. 각각이 학습 자료 안에 있는 데이터($x$)를 초기 가중치($w$)와 곱한 값($z$)을 구한다.\n",
    "3. 2에서 구한 값을 활성화 함수(예를 들어 양극성 계단 함수)에 넣어 예측 값($\\hat{y}$)를 구한다.\n",
    "4. 예측값과 실제값을 비교하여 새로운 가중치 값($\\Delta{w}$)을 만든다.\n",
    "5. 새로운 가중치 값($w$=$w$+$\\Delta{w}$)과 다음 학습 자료 안에 있는 데이터를 곱한 값($z$)을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/Perceptron-process.png?raw=true\" width=800></img>\n",
    "<center>그림 4: 퍼셉트론 알고리즘의 전체 과정</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론이 비록 70년 이상된 알고리즘이나,두 클래스가 선형으로 분류 가능하다면  이 알고리즘은 하나의 퍼셉트론으로도 가중치를 찾을 수 있도록 멋지게 수렴합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 퍼셉트론 알고리즘의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 배타적논리합$^{XOR}$\n",
    "1957년 코넬 항공 연구소의 프랭크 로젠블랏트가 퍼셉트론 알고리즘을 발표하고 실제로 실행하는 컴퓨터(기계?)를 제작했을때, 뉴욕타임즈는 가까운 미래에 말하고, 생각하고, 걸을 수 있는 컴퓨터의 세상이 도래할 것이라는 미래 예측 기사를 냈습니다.  그러나 1969년, 퍼셉트론으로는 배타적논리합$^{XOR}$ 문제도 풀 수 없다는 사실을 MIT 미디어 랩의 창시자이며, 인공지능의 아버지라고 불리는 마빈 민스키$^{Marvin \\ Minsky}$교수가 증명하였습니다.  다만, 다층 퍼셉트론(MLP: Multi-Layer Perceptron)을 학습시킬 수 있다면, XOR문제를 풀 수 있지만, 그 방법이 없다고 했습니다.  그런데, 1974년, 당시 하버드 대학의 박사과정 학생이었던 펄 워보스가 최초로 MLP를 학습시킬 수 있는 오류 역전파(Backpropagation)을 발표하면서, 기계학습은 새로운 전기를 맞게 되었습니다.  이 알고리즘에 대해서 우리도 곧 다룰 것입니다. 기대하십시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배타적논리합의 경우, 두 입력값이 같으면 0이며 두 입력값이 다를 경우 1을 반환해야 합니다. 두 입력값의 경우를 그래프로 나타내보면 아래 왼쪽 그림과 같이 나타납니다. 그러나 해당 그래프는 선형적으로 분류될 수 없음을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XORClassifier3.png?raw=true\" width=\"600\"></img>\n",
    "<center>그림 5: 퍼셉트론 알고리즘의 한계 (베타적논리합)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론 알고리즘을 실행할 때, 입력되는 학습자료가 선형으로 분류 가능하지 않을 수 있으므로, 오차의 한계를 설정하거나 학습의 최대 반복 횟수(에포크$^{epoch}$)를 미리 정하는 것이 좋습니다. 그렇지 않다면 퍼셉트론 알고리즘은 영원히 반복되며, 가중치를 이상한 방향으로 학습해 나갈 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 최적화된 가중치\n",
    "\n",
    "또 한가지 퍼셉트론의 한계는 아래 왼쪽 그림과 같이 두 클래스를 분류한다고 할 때 퍼셉트론 알고리즘은 두 직선에 사이 존재하는 하나의 직선으로 수렴하지만, 두 직선 사이에 존재하는 최적의 직선을 선택할 수는 없습니다. 오른쪽 그림과 같이 하나의 새로운 값이 들어온다고 했을 때, 이 입력 값을 분류하기 위해서는 또 새로운 직선이 만들어지기 때문이지요. 따라서 어떠한 직선이 모든 입력을 구분하는 최적의 직선이라고 퍼셉트론은 말할 수 없습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/Limit_Perceptron.png?raw=true\" width=\"800\">\n",
    "<center>그림 3: 퍼셉트론 분류기의 한계</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론을 계속 학습하여, 가중치를 변화시키더라도, 어떤 가중치 값들이 가장 이상적으로 모든 값(학습 데이터와 새로운 데이터)들을 분류해는지 알 수 없습니다. 실제 새로운 데이터를 넣어보기 전까지는 퍼셉트론은 학습된 데이터에 한해, 분류를 할 수 있다 없다만 알려줄 뿐입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 퍼셉트론 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위에서 배운 퍼셉트론을 갖고 실제로 퍼셉트론을 만들고 학습 자료를 구분해보도록 합시다.\n",
    "\n",
    "그림 4-1과 같이 6개의 학습(훈련) 자료가 주어졌다고 가정합시다.  각 자료의 클래스 레이블은 `y = [1, -1, -1, -1, 1, 1]`입니다. 학습 자료들을 사용하여 이진 페셉트론을 학습시키고, 학습 자료들을 이진 분류할 수 있는 가중치를 구하고, 그림 4-2와 같이 시각화하는 것이 목표입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td><img src='https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/perceptronExample1.png?raw=true' width=\"400\"> \n",
    "        <center> 그림 4-1: 퍼셉트론 예제 1</center> </td>\n",
    "    <td><img src='https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/perceptronExample1Result.png?raw=true' width=\"250\"> \n",
    "        <center> 그림 4-2: 퍼셉트론 예제 1 결과($\\eta=0.1$)</center></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "이진 퍼셉트론의 가중치$\\mathbf{w}$를 구하는 계산을 직접 손으로 계산하면서 아래 표를 완성해 봅시다. 단, 초기 가중치는 계산의 편의를 위하여 $\\mathbf{w^T} = [0 \\ \\ 1 \\ \\ 0.5]$으로 설정합니다.  학습률은 $\\eta = 0.1$, 학습자료는 1번 즉 $\\mathbf{x}^{(1)}= [1, 1]$부터 차례로 입력합니다. \n",
    "\n",
    "표에 나온 값은 $\\mathbf{x}^{(1)}= [1, 1]$에 bias 까지 추가한 상태입니다. 즉, bias를 b로 나타내지 않고 $w_0$$x_0$으로 나타낸 것입니다. 위에서 가중치 백터의 첫번 째 값이 0이므로 $w_0$를 0으로, $x_0$를 1로 둔 것으로 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                      표 1: 각 학습 자료의 입력에 따른 가중치 계산 과정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $i$   |  $(x_0^{(i)}, x_1^{(i)}, x_2^{(i)})$ |  $(w_0, w_1, w_2)$           | $\\mathbf{w^Tx}$ | $\\hat{y}^{(i)}$  | $y^{(i)}$ |  $\\eta$ |$\\Delta w$ |\n",
    "| ----  |    ---         |  --  |:---------------:|:----------------:|:----------:|:--------:|:-----------:|\n",
    "|  1    |  (1.0, 1.0, 1.0)  |(0.0, 1.0, 0.5) | $\\hspace{60pt}$ |  $\\hspace{30pt}$ |     1        |  0.1    | $\\hspace{60pt}$ \n",
    "|  2    |  (1.0, 2.0, -2.0) |                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  3    |  (1.0, -1.0,-1.5) |                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  4    |  (1.0, -2.0, -1.0)|                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  5    |  (1.0, -2.0, 1.0) |                 |                 |                 |     1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  6    |  (1.0, 1.5, -0.5) |                 |                 |                 |     1        |  0.1   |$\\hspace{60pt}$ \n",
    "| final |     -             | $\\hspace{60pt}$ |       -         |        -        |     -        |    -    |      -        |       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째로 $i$가 1일 경우, $\\Delta w$의 값을 구해보도록 합시다.\n",
    "\n",
    "$x^{(1)}$ = (1.0, 1.0, 1.0) 이고, $w^{T}$값은 (0.0, 1.0, 0.5)이므로 두 벡터의 내적 값 $\\mathbf{w^Tx}$ 은 (1.0 $\\times$ 0.0) + (1.0 $\\times$ 1.0) + (1.0 $\\times$ 0.5) = 1.5가 됩니다.\n",
    "\n",
    "활성화 함수가 양극성 계단 함수라고 할 때, 1.5는 1로 변환됩니다. 따라서  $\\hat{y}^{(1)}$의 값은 1이 됩니다. \n",
    "기댓값 $\\hat{y}^{(1)}$과 실제값 $y$의 값이 같기 때문에 $\\Delta w$ = 0.1$\\times$(1 - 1) = 0 이 됩니다. \n",
    "\n",
    "첫 번째 입력 값은 가중치를 변화시키지 못했습니다. 이때 착각해서는 안되는 점은, 가중치가 변하지 않았다고 하여, 해당 입력 값이 쓸모 없는 것은 절대로 아닙니다. 가중치가 변하지 않았다는 것은 현재 가중치로 입력 값을 잘 분류할 수 있다는 뜻을 입증하는 근거가 되기 때문이지요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론이 첫 번째 학습한 후, 표에 채워지는 내용은 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                      표 1-1: 각 학습 자료의 입력에 따른 가중치 계산 과정 \n",
    "\n",
    "| $i$   |  $x^{(i)}$       |  $ w $           | $\\mathbf{w^Tx}$ | $\\hat{y}^{(i)}$  | $y^{(i)}$ |  $\\eta$ |$\\Delta w$ |\n",
    "|:------:|:-----------------:|:--------------:|:---------------:|:----------------:|:----------:|:--------:|:-----------:|\n",
    "|  1     |  (1.0, 1.0, 1.0)  |(0.0, 1.0, 0.5) | 1.5             |           1 |     1        |  0.1    | (0.0, 0.0, 0.0)\n",
    "|  2     |  (1.0, 2.0, -2.0) |                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  3     |  (1.0, -1.0,-1.5) |                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  4     |  (1.0, -2.0, -1.0)|                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  5     |  (1.0, -2.0, 1.0) |                 |                 |                 |     1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  6     |  (1.0, 1.5, -0.5) |                 |                 |                 |     1        |  0.1   |$\\hspace{60pt}$ \n",
    "| final  |     -             | $\\hspace{60pt}$ |       -         |        -        |     -        |    -    |      -        |       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째로 $i$가 2일 경우, $\\Delta w$의 값을 구해보도록 합시다.\n",
    "\n",
    "$x^{(2)}$ = (1.0, 2.0, -2.0) 이고, $w^{T}$값은 첫 번째 경우에서 변하지 않았기 때문에 (0.0, 1.0, 0.5)입니다. 두 벡터의 내적 값 $\\mathbf{w^Tx}$ 은 (1.0 $\\times$ 0.0) + (2.0 $\\times$ 1.0) + (-2.0 $\\times$ 0.5) = 1가 됩니다.\n",
    "\n",
    "활성화 함수가 양극성 계단 함수라고 할 때, 1은 1로 변환됩니다. 따라서  $\\hat{y}^{(1)}$의 값은 1이 됩니다. \n",
    "기댓값 $\\hat{y}^{(1)}$과 실제값 $y$의 값과 다르기 때문에 $\\Delta w$ = 0.1$\\times$(-1 - 1) = -0.2 이 됩니다. \n",
    "\n",
    "이제 가중치 값을 변화시킬 차례입니다. \n",
    "\n",
    "\\begin{align}\n",
    "   \\Delta w_j= \\eta (y^{(i)} - \\hat{y}^{(i)})x_j^{(i)}   \\tag{1} \n",
    "\\end{align}\n",
    "\n",
    "이기 때문에, $x^{(2)}$의 원소들의 값에 -0.2를 곱하면 (-0.2, -0.4, 0.4)가 됩니다.\n",
    "\n",
    "퍼셉트론이 두 번째 학습한 후, 표에 채워지는 내용은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                      표 1-1: 각 학습 자료의 입력에 따른 가중치 계산 과정 \n",
    "\n",
    "| $i$   |  $x^{(i)}$       |  $ w $           | $\\mathbf{w^Tx}$ | $\\hat{y}^{(i)}$  | $y^{(i)}$ |  $\\eta$ |$\\Delta w$ |\n",
    "|:------:|:-----------------:|:--------------:|:---------------:|:----------------:|:----------:|:--------:|:-----------:|\n",
    "|  1     |  (1.0, 1.0, 1.0)  |(0.0, 1.0, 0.5) | 1.5             |           1 |     1        |  0.1    | (0.0, 0.0, 0.0)\n",
    "|  2     |  (1.0, 2.0, -2.0) | (0.0, 1.0, 0.5) | 1                |       1          |    -1        |  0.1   | (-0.2, -0.4, 0.4) \n",
    "|  3     |  (1.0, -1.0,-1.5) |                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  4     |  (1.0, -2.0, -1.0)|                 |                 |                 |    -1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  5     |  (1.0, -2.0, 1.0) |                 |                 |                 |     1        |  0.1   |$\\hspace{60pt}$ \n",
    "|  6     |  (1.0, 1.5, -0.5) |                 |                 |                 |     1        |  0.1   |$\\hspace{60pt}$ \n",
    "| final  |     -             | $\\hspace{60pt}$ |       -         |        -        |     -        |    -    |      -        |       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여섯 번째까지 학습한 후에 표에 채워지는 내용은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| $i$   |  $x^{(i)}$       |  $ w $            | $\\mathbf{w^Tx}$ | $\\hat{y}^{(i)}$  | $y^{(i)}$ | $\\eta$ | $\\Delta w$\n",
    "|:------:|:-----------------:|:---------------:|:---------------:|:----------------:|:----------:|:--------:|:-------------:|\n",
    "|  1     |  (1.0, 1.0, 1.0)  | (0.0, 1.0, 0.5)  |  1.5            |        1         |     1        |   0.1   |     0\n",
    "|  2     |  (1.0, 2.0, -2.0) | (0.0, 1.0, 0.5)  |  1.0            |        1         |    -1        |   0.1  | (-0.2,-0.4,0.4) \n",
    "|  3     |  (1.0, -1.0,-1.5) |(-0.2, 0.6, 0.9)  |   -2.15         |       -1         |    -1        |   0.1  |   0\n",
    "|  4     |  (1.0, -2.0, -1.0)|(-0.2, 0.6, 0.9)  |   -2.3          |       -1          |    -1        |  0.1  |   0\n",
    "|  5     |  (1.0, -2.0, 1.0) |(-0.2, 0.6, 0.9)  |   -0.5          |       -1          |     1        | 0.1   | (0.2, -0.4, 0.2)\n",
    "|  6     |  (1.0, 1.5, -0.5) | (0.0, 0.2, 1.1)  |   -0.25         |       -1          |     1        | 0.1   | (0.2, 0.3, -0.1)   \n",
    "| final  |     -             | (0.2, 0.5, 1.0) |  $\\hspace{60pt}$ |       -         |        -        |   -    |     $\\hspace{60pt}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "여기서는 우리가 Step 1에서 산출한 가중치을 다음 공식에 대입하여 판별식을 $x_2$에 관한 식으로 구합니다. 가중치로부터 학습 자료들을 두 클래스로 나누는 판별식$^{decision \\ boundary}$은 $\\mathbf{w^Tx} = 0$입니다.  우리가 산출한 가중치 `w = [0.2, 0.5, 1.0]` 입니다. 그러므로, \n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{w^Tx} &= 0  \\\\\n",
    "\\begin{bmatrix}\n",
    "w_0&w_1&w_2\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ x_1 \\\\ x_2\n",
    "\\end{bmatrix} &= 0 \\\\\n",
    "  w_0 + w_1x_1 + w_2x_2 &= 0  \\\\\n",
    "  0.2 + 0.5x_1 + 1.0x_2 &= 0  \\\\\n",
    "  x_2 &= -0.5x_1 - 0.2\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "우리가 Step 1에서 산출한 가중치(0.2, 0.5, 1.0)로 `plot_xyw()` 함수를 호출하여 판별식을 학습 자료와 함께 시각화합니다. \n",
    "\n",
    "`plot_xyw()` 함수에서 요구하는 X는 학습(훈련) 자료들이며, y는 각 학습 자료의 클래스 레이블입니다. `W`는 선택적 인자이며, 학습 자료를 이진 분류할 수 있는 가중치 즉 판별식입니다. `annotate`는 각 학습 자료들 일련 번호 표시 여부를 말합니다. 그림을 저장하길 원한다면 `savefig`에 파일 이름을 설정할 수 있습니다.  `plot_xyw()` 함수의 코드에 대한 설명은 다음 강의에서 하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "%run code/plot_xyw.py\n",
    "\n",
    "x = np.array([[1.0, 1.0], [2.0, -2.0], [-1.0, -1.5], [-2.0, -1.0], [-2.0,1.0], [1.5, -0.5]]) \n",
    "X = np.c_[ np.ones(len(x)), x ]\n",
    "y = np.array([1, -1, -1, -1, 1, 1])\n",
    "w = np.array([0.2, 0.5, 1.0])\n",
    "plot_xyw(X, y, w, X0=True, annotate=True, savefig='images/perceptronExample1Step1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드를 한 줄 씩 설명하자면, \n",
    "- `x = np.array([[1.0, 1.0], [2.0, -2.0], [-1.0,-1.5], [-2.0, -1.0], [-2.0,1.0], [1.5, -0.5]]) ` : bias를 뺀 x값을 배열로 나타낸 코드입니다.\n",
    "- `X = np.c_[ np.ones(len(x)), x ]` : bias값으로 1.0을 넣어주는 코드입니다.\n",
    "- `y = np.array([1, -1, -1, -1, 1, 1])` : 각 데이터들의 라벨을 결정해주는 코드입니다.\n",
    "- `w = np.array([0.2, 0.5, 1.0])` : 위에서 최종적으로 구한 가중치 값의 배열을 만드는 코드입니다.\n",
    "- `plot_xyw(X, y, w, X0=True, annotate=True, savefig='images/perceptronExample1Result.png')` : 위에서 나온 데이터들을 시각화 하기 위한 함수를 부르는 코드입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저희가 구한 마지막 가중치 값이 확실하게 입력 데이터을 분류하는 것을 그래프로 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가로, 위에서 보여드린 `plot_xyw` 함수를 사용해서 각 단계마다 가중치의 변화, 즉 결정 경계가 어떻게 변화하는지 그려보았습니다. 다음 반복 횟수가 많아지면 많아질 수록, 가중치(w)값의 변화와 더불어 입력 값을 더 정확하게 분류하는 것을 아래의 표시된 4 개의 그래프로 확인할 수 있습니다.\n",
    "\n",
    "결론적으로, 이렇게 간단한 알고리즘으로 6개의 입력값을 완벽하게 분류하는 경계선을 찾아냈습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) | (2) | (3) | (4)\n",
    "- | - \n",
    "![alt](https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/perceptronExample1Result_1.png?raw=true) | ![alt](https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/perceptronExample1Result_3.png?raw=true) | ![alt](https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/perceptronExample1Result_6.png?raw=true) | ![alt](https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/perceptronExample1Result_7.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chekcing it by code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "%run code/plot_xyw.py\n",
    "\n",
    "x = np.array([[1.0, 1.0], [2.0, -2.0], [-1.0, -1.5], [-2.0, -1.0], [-2.0,1.0], [1.5, -0.5]]) \n",
    "X = np.c_[ np.ones(len(x)), x ]\n",
    "y = np.array([1, -1, -1, -1, 1, 1])\n",
    "w = np.array([0.0, 1.0, 0.5])\n",
    "\n",
    "eta = 0.1\n",
    "for i, (xi, yi) in enumerate(zip(X, y)):\n",
    "    z = np.dot(xi, w)                              # Compute net input, same as np.dot(w.T, x)\n",
    "    yhat = np.where(z >= 0.0, 1, -1)          # Apply step func and get output\n",
    "    delta = eta * (yi - yhat) * xi                 # Compute delta    \n",
    "    w += delta                                     # Adjust weight\n",
    "    print('{} z={}, \\t y:{}, yhat:{}, delta{}, w:{}'\n",
    "          .format(i+1, np.round(z,2), yi, yhat, delta, w))\n",
    "\n",
    "print('w = ', w)\n",
    "plot_xyw(X, y, w, X0=True, annotate=True, savefig='images/perceptronExample1Step1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
